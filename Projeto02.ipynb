{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Classificador Automático de Sentimento\n",
    "\n",
    "Você foi contratado por uma empresa parar analisar como os clientes estão reagindo a um determinado produto no Twitter. A empresa deseja que você crie um programa que irá analisar as mensagens disponíveis e classificará como \"relevante\" ou \"irrelevante\". Com isso ela deseja que mensagens negativas, que denigrem o nome do produto, ou que mereçam destaque, disparem um foco de atenção da área de marketing.<br /><br />\n",
    "Como aluno de Ciência dos Dados, você lembrou do Teorema de Bayes, mais especificamente do Classificador Naive-Bayes, que é largamente utilizado em filtros anti-spam de e-mails. O classificador permite calcular qual a probabilidade de uma mensagem ser relevante dadas as palavras em seu conteúdo.<br /><br />\n",
    "Para realizar o MVP (*minimum viable product*) do projeto, você precisa implementar uma versão do classificador que \"aprende\" o que é relevante com uma base de treinamento e compara a performance dos resultados com uma base de testes.<br /><br />\n",
    "Após validado, o seu protótipo poderá também capturar e classificar automaticamente as mensagens da plataforma.\n",
    "\n",
    "## Informações do Projeto\n",
    "\n",
    "Prazo: 13/Set até às 23:59.<br />\n",
    "Grupo: 1 ou 2 pessoas.<br /><br />\n",
    "Entregáveis via GitHub: \n",
    "* Arquivo notebook com o código do classificador, seguindo as orientações abaixo.\n",
    "* Arquivo Excel com as bases de treinamento e teste totalmente classificado.\n",
    "\n",
    "**NÃO disponibilizar o arquivo com os *access keys/tokens* do Twitter.**\n",
    "\n",
    "\n",
    "### Check 3: \n",
    "\n",
    "Até o dia 06 de Setembro às 23:59, o notebook e o xlsx devem estar no Github com as seguintes evidências: \n",
    "    * Conta no twitter criada.\n",
    "    * Produto escolhido.\n",
    "    * Arquivo Excel contendo a base de treinamento e teste já classificado.\n",
    "\n",
    "Sugestão de leitura:<br />\n",
    "http://docs.tweepy.org/en/v3.5.0/index.html<br />\n",
    "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Preparando o ambiente\n",
    "\n",
    "Instalando a biblioteca *tweepy* para realizar a conexão com o Twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy\n",
    "\n",
    "!pip install sklearn-pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as Bibliotecas que serão utilizadas. Esteja livre para adicionar outras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Autenticando no  Twitter\n",
    "\n",
    "Para realizar a captura dos dados é necessário ter uma conta cadastrada no twitter:\n",
    "\n",
    "* Conta: ***@tainarasm98***\n",
    "\n",
    "\n",
    "1. Caso ainda não tenha uma: https://twitter.com/signup\n",
    "1. Depois é necessário registrar um app para usar a biblioteca: https://apps.twitter.com/\n",
    "1. Dentro do registro do App, na aba Keys and Access Tokens, anotar os seguintes campos:\n",
    "    1. Consumer Key (API Key)\n",
    "    1. Consumer Secret (API Secret)\n",
    "1. Mais abaixo, gere um Token e anote também:\n",
    "    1. Access Token\n",
    "    1. Access Token Secret\n",
    "    \n",
    "1. Preencha os valores no arquivo \"auth.pass\"\n",
    "\n",
    "**ATENÇÃO**: Nunca divulgue os dados desse arquivo online (GitHub, etc). Ele contém as chaves necessárias para realizar as operações no twitter de forma automática e portanto é equivalente a ser \"hackeado\". De posse desses dados, pessoas mal intencionadas podem fazer todas as operações manuais (tweetar, seguir, bloquear/desbloquear, listar os seguidores, etc). Para efeito do projeto, esse arquivo não precisa ser entregue!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dados de autenticação do twitter:\n",
    "\n",
    "#Coloque aqui o identificador da conta no twitter: @tainarasm98\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "with open('auth.pass') as fp:    \n",
    "    data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. Não modificar\n",
    "auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Coletando Dados\n",
    "\n",
    "Agora vamos coletar os dados. Tenha em mente que dependendo do produto escolhido, não haverá uma quantidade significativa de mensagens, ou ainda poder haver muitos retweets.<br /><br /> \n",
    "Configurando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Produto escolhido:\n",
    "produto = \"Ruffles\"\n",
    "\n",
    "#Quantidade mínima de mensagens capturadas:\n",
    "n = 500\n",
    "#Quantidade mínima de mensagens para a base de treinamento:\n",
    "t = 300\n",
    "\n",
    "#Filtro de língua, escolha uma na tabela ISO 639-1.\n",
    "lang = 'pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturando os dados do twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cria um objeto para a captura\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#Inicia a captura, para mais detalhes: ver a documentação do tweepy\n",
    "i = 1\n",
    "msgs = []\n",
    "for msg in tweepy.Cursor(api.search, q=produto, lang=lang).items():    \n",
    "    msgs.append(msg.text.lower())\n",
    "    i += 1\n",
    "    if i > n:\n",
    "        break\n",
    "\n",
    "#Embaralhando as mensagens para reduzir um possível viés\n",
    "shuffle(msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando os dados em uma planilha Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Verifica se o arquivo não existe para não substituir um conjunto pronto\n",
    "if not os.path.isfile('./{0}.xlsx'.format(produto)):\n",
    "    \n",
    "    #Abre o arquivo para escrita\n",
    "    writer = pd.ExcelWriter('{0}.xlsx'.format(produto))\n",
    "    \n",
    "    #divide o conjunto de mensagens em duas planilhas\n",
    "    dft = pd.DataFrame({'Treinamento' : pd.Series(msgs[:t])})\n",
    "    dft.to_excel(excel_writer = writer, sheet_name = 'Treinamento', index = False)\n",
    "\n",
    "    dfc = pd.DataFrame({'Teste' : pd.Series(msgs[t:])})\n",
    "    dfc.to_excel(excel_writer = writer, sheet_name = 'Teste', index = False)\n",
    "\n",
    "    #fecha o arquivo\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Classificando as Mensagens\n",
    "\n",
    "Agora você deve abrir o arquivo Excel com as mensagens capturadas e classificar na Coluna B se a mensagem é relevante ou não.<br /> \n",
    "Não se esqueça de colocar um nome para a coluna na célula **B1**.<br /><br />\n",
    "Fazer o mesmo na planilha de Controle.\n",
    "\n",
    "___\n",
    "## Montando o Classificador Naive-Bayes\n",
    "\n",
    "Com a base de treinamento montada, comece a desenvolver o classificador. Escreva o seu código abaixo:\n",
    "\n",
    "Opcionalmente: \n",
    "* Limpar as mensagens removendo os caracteres: enter, :, \", ', (, ), etc. Não remover emojis.<br />\n",
    "* Corrigir separação de espaços entre palavras e/ou emojis.\n",
    "* Propor outras limpezas/transformações que não afetem a qualidade da informação.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Irrelevante' 'Irrelevante' 'Relevante' 'Relevante' 'Irrelevante'\n",
      " 'Relevante' 'Irrelevante' 'Relevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Relevante' 'Irrelevante' 'Relevante' 'Irrelevante'\n",
      " 'Relevante' 'Irrelevante' 'Relevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Relevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Relevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Relevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Relevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Relevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Relevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Relevante'\n",
      " 'Irrelevante' 'Relevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Relevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Relevante' 'Relevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Relevante' 'Relevante' 'Relevante' 'Irrelevante' 'Relevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Relevante' 'Relevante'\n",
      " 'Relevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Relevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Relevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Relevante' 'Relevante' 'Relevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Relevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Relevante' 'Relevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Relevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Relevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Relevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Relevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Relevante' 'Irrelevante' 'Irrelevante' 'Relevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Relevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Relevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Relevante' 'Relevante' 'Irrelevante' 'Relevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Relevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Relevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Relevante' 'Irrelevante']\n",
      "['Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante'\n",
      " 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante' 'Irrelevante']\n",
      "0.676666666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  Relevante       0.30      0.62      0.40        53\n",
      "Irrelevante       0.89      0.69      0.78       247\n",
      "\n",
      "avg / total       0.79      0.68      0.71       300\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predito</th>\n",
       "      <th>Irrelevante</th>\n",
       "      <th>Relevante</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Real</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Irrelevante</th>\n",
       "      <td>170</td>\n",
       "      <td>77</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Relevante</th>\n",
       "      <td>20</td>\n",
       "      <td>33</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>190</td>\n",
       "      <td>110</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predito      Irrelevante  Relevante  All\n",
       "Real                                    \n",
       "Irrelevante          170         77  247\n",
       "Relevante             20         33   53\n",
       "All                  190        110  300"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xls_file = pd.ExcelFile('Ruffles.xlsx')\n",
    "\n",
    "df = xls_file.parse('Treinamento')\n",
    "df2 = xls_file.parse('Teste')\n",
    "\n",
    "\n",
    "treinamento = pd.DataFrame({'Treinamento': df['Treinamento']})\n",
    "classificacao = pd.DataFrame({'Classificação': df['Classificação']})\n",
    "teste = pd.DataFrame({'Teste': df2['Teste']})\n",
    "                            \n",
    "treinamento['Tokenized_treinamento'] = treinamento.apply(lambda row: nltk.word_tokenize(row['Treinamento']), axis=1)\n",
    "\n",
    "\n",
    "stop = stopwords.words('portuguese')\n",
    "\n",
    "#treinamento = \" \".join(re.findall('[A-Z][^A-Z]*', treinamento)) - importante vooos\n",
    "\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "tokenized_treinamento_sem_pontuacao = []\n",
    "\n",
    "for review in treinamento['Tokenized_treinamento']:\n",
    "    new_review = []\n",
    "    for token in review: \n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "\n",
    "    tokenized_treinamento_sem_pontuacao.append(new_review)\n",
    "    \n",
    "\n",
    "tokenized_treinamento_sem_stopwords = []\n",
    "for frase in tokenized_treinamento_sem_pontuacao:\n",
    "    new_term_vector = []\n",
    "    for palavra in frase:\n",
    "        if not palavra in stopwords.words('portuguese'):\n",
    "            new_term_vector.append(palavra)\n",
    "    tokenized_treinamento_sem_stopwords.append(new_term_vector)\n",
    "\n",
    "    \n",
    "treinamento['Tokenized_treinamento'] = tokenized_treinamento_sem_stopwords\n",
    "\n",
    "\n",
    "\n",
    "#\"\".join(treinamento['Tokenized_treinamento'])\n",
    "\n",
    "#d = MosesDetokenizer()\n",
    "#detokens = d.detokenize(tokenized_treinamento_sem_stopwords)\n",
    "#detokens = \"\".join(detokens)\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', tokenizer=lambda doc: doc, lowercase=False)\n",
    "freq_tweets = vectorizer.fit_transform(treinamento['Tokenized_treinamento'])\n",
    "\n",
    "#freq_tweets = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False).fit_transform(treinamento['Tokenized_treinamento'])\n",
    "freq_tweets.toarray()\n",
    "modelo = MultinomialNB(alpha=1)\n",
    "modelo.fit(freq_tweets,classificacao['Classificação'])\n",
    "print(modelo.predict(freq_tweets))\n",
    "\n",
    "\n",
    "freq_testes = vectorizer.transform(teste['Teste'])\n",
    "print(modelo.predict(freq_testes))\n",
    "\n",
    "\n",
    "resultados = cross_val_predict(modelo, freq_tweets, classificacao['Classificação'], cv=10)\n",
    "print(metrics.accuracy_score(classificacao['Classificação'],resultados))\n",
    "\n",
    "sentimento=['Relevante','Irrelevante']\n",
    "print(metrics.classification_report(classificacao['Classificação'],resultados,sentimento))\n",
    "\n",
    "pd.crosstab(classificacao['Classificação'], resultados, rownames=['Real'], colnames=['Predito'], margins=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Irrelevante    80.5\n",
       "Relevante      19.5\n",
       "Name: Classificação, dtype: float64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xls_file = pd.ExcelFile('Ruffles.xlsx')\n",
    "\n",
    "df = xls_file.parse('Treinamento')\n",
    "df2 = xls_file.parse('Teste')\n",
    "\n",
    "treinamento = pd.DataFrame({'Treinamento': df['Treinamento'],\n",
    "                            'Classificação': df['Classificação']})\n",
    "                            \n",
    "treinamento['Tokenized_treinamento'] = treinamento.apply(lambda row: nltk.word_tokenize(row['Treinamento']), axis=1)\n",
    "\n",
    "relevante = treinamento[(treinamento.Classificação == 'Relevante')]\n",
    "irrelevante = treinamento[(treinamento.Classificação ==  'Irrelevante')]\n",
    "\n",
    "\n",
    "\n",
    "all_words = []\n",
    "all_words2 = []\n",
    "frequencia = {}\n",
    "frequencia2 = {}\n",
    "prob_palavra_relevante = {}\n",
    "prob_palavra_irrelevante = {}\n",
    "\n",
    "\n",
    "for percorre_tweets in relevante['Tokenized_treinamento']:\n",
    "    all_words.extend(percorre_tweets)\n",
    "\n",
    "for percorre_tweets in irrelevante['Tokenized_treinamento']:\n",
    "    all_words2.extend(percorre_tweets)\n",
    "\n",
    "for i in all_words:\n",
    "    frequencia[i] = all_words.count(i)\n",
    "\n",
    "for i in all_words2:\n",
    "    frequencia2[i] = all_words2.count(i)   \n",
    "\n",
    "for j in frequencia:\n",
    "    prob_palavra_relevante[j] = ((frequencia[j]+1)/(len(all_words) + (len(frequencia)+len(frequencia2))))\n",
    "\n",
    "for j in frequencia2:\n",
    "    prob_palavra_irrelevante[j] = ((frequencia2[j]+1)/(len(all_words) + (len(frequencia)+len(frequencia2))))\n",
    "\n",
    "prob_rel_irr = df.Classificação.value_counts(True)\n",
    "\n",
    "prob_teste = df2.Classificação.value_counts(True)*100\n",
    "\n",
    "prob_teste\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Verificando a performance\n",
    "\n",
    "Agora você deve testar o seu Classificador com a base de Testes.<br /><br /> \n",
    "\n",
    "Você deve extrair as seguintes medidas:\n",
    "* Porcentagem de positivos falsos (marcados como relevante mas não são relevantes)\n",
    "* Porcentagem de positivos verdadeiros (marcado como relevante e são relevantes)\n",
    "* Porcentagem de negativos verdadeiros (marcado como não relevante e não são relevantes)\n",
    "* Porcentagem de negativos falsos (marcado como não relevante e são relevantes)\n",
    "\n",
    "Opcionalmente:\n",
    "* Criar categorias intermediárias de relevância baseado na diferença de probabilidades. Exemplo: muito relevante, relevante, neutro, irrelevante e muito irrelevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Positivos Verdadeiros': 7.000000000000001, 'Negativos Verdadeiros': 80.5, 'Positivos Falsos': 0.0, 'Negativos Falsos': 12.5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"for i in range(len(classificacao_teste)):\\n    if classificacao_teste[i]=='Relevante':\\n        print(tweets[i])\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### teste = pd.DataFrame({'Teste': df2['Teste']})\n",
    "                            \n",
    "teste['Tokenized_teste'] = teste.apply(lambda row: nltk.word_tokenize(row['Teste']), axis=1)\n",
    "\n",
    "relevante_irrelevante = []\n",
    "\n",
    "tweets = []\n",
    "\n",
    "\n",
    "for tweet in teste['Tokenized_teste']:\n",
    "    tweets.append(tweet)\n",
    "\n",
    "        \n",
    "def multiply(numbers):  \n",
    "    total = 1\n",
    "    for x in numbers:\n",
    "        total *= x  \n",
    "    return total\n",
    "\n",
    "classificacao_teste = []\n",
    "diferenca_probabilidades = []\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    sentenca_relevante = []\n",
    "    sentenca_irrelevante = []\n",
    "    relevante = []\n",
    "    irrelevante = []\n",
    "    for j in tweets[i]:\n",
    "        if j in frequencia:\n",
    "            sentenca_relevante.append(((frequencia[j])/(len(all_words)))*prob_rel_irr['Relevante'])\n",
    "        if j not in frequencia:\n",
    "            sentenca_relevante.append((1/(len(all_words) + (len(frequencia)+len(frequencia2))))*prob_rel_irr['Relevante'])\n",
    "            \n",
    "        if j in frequencia2:\n",
    "            sentenca_irrelevante.append(((frequencia2[j])/(len(all_words2)))*prob_rel_irr['Irrelevante'])\n",
    "        if j not in frequencia2:\n",
    "            sentenca_irrelevante.append((1/(len(all_words2) + (len(frequencia)+len(frequencia2))))*prob_rel_irr['Irrelevante'])\n",
    "        \n",
    "    \n",
    "    relevante.append(multiply(sentenca_relevante))\n",
    "    irrelevante.append(multiply(sentenca_irrelevante))\n",
    "    \n",
    "    for l in range(len(relevante)):\n",
    "        if relevante[l]>irrelevante[l]:\n",
    "            classificacao_teste.append('Relevante')\n",
    "        else:\n",
    "            classificacao_teste.append('Irrelevante')\n",
    "            \n",
    "        \n",
    "    \n",
    "verifica_performance = {}\n",
    "soma = 0\n",
    "soma_1 = 0\n",
    "soma_2 = 0\n",
    "soma_3 = 0\n",
    "    \n",
    "for i in range(len(df2.Classificação)):\n",
    "    if df2.Classificação[i] == classificacao_teste[i]:\n",
    "        if classificacao_teste[i] == 'Relevante':\n",
    "            soma+=1\n",
    "        verifica_performance['Positivos Verdadeiros'] = ((soma/200)*100)\n",
    "        \n",
    "        if classificacao_teste[i] == 'Irrelevante':\n",
    "            soma_1+=1\n",
    "        verifica_performance['Negativos Verdadeiros'] = (soma_1/200)*100\n",
    "    else:\n",
    "        if classificacao_teste[i] == 'Relevante':\n",
    "            soma_2+=1\n",
    "        verifica_performance['Positivos Falsos'] = (soma_2/200)*100\n",
    "        \n",
    "        if classificacao_teste[i] == 'Irrelevante':\n",
    "            soma_3+=1\n",
    "        verifica_performance['Negativos Falsos'] = (soma_3/200)*100\n",
    "    \n",
    "           \n",
    "\n",
    "print(verifica_performance)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"for i in range(len(classificacao_teste)):\n",
    "    if classificacao_teste[i]=='Relevante':\n",
    "        print(tweets[i])\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "___\n",
    "## Concluindo\n",
    "\n",
    "Escreva aqui a sua conclusão.<br /> \n",
    "Faça um comparativo qualitativo sobre as medidas obtidas.<br />\n",
    "Explique como são tratadas as mensagens com dupla negação e sarcasmo.<br />\n",
    "Proponha um plano de expansão. Por que eles devem continuar financiando o seu projeto?<br />\n",
    "\n",
    "Opcionalmente: \n",
    "* Discorrer por que não posso alimentar minha base de Treinamento automaticamente usando o próprio classificador, aplicado a novos tweets.\n",
    "* Propor diferentes cenários de uso para o classificador Naive-Bayes. Cenários sem intersecção com este projeto.\n",
    "* Sugerir e explicar melhorias reais no classificador com indicações concretas de como implementar (não é preciso codificar, mas indicar como fazer e material de pesquisa sobre o assunto).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso propósito inicial era classificar os tweets relacionados à marca Lay's (batata), no entanto, após classificar todas as mensagens da base de treinamento e controle, percebemos que não haviam tweets os quais pudéssemos considerar como relevantes, senão os publicados por um mesmo usuário, sendo que o restante se constituía em retweets das postagens deste. Resolvemos, então, classificar as mensagens relativas a outro produto: a batata Ruffles. Isso explica por que o arquivo Excel 'comitado' no Git para o primeiro Check se refere à marca Lay's e não a esta.\n",
    "\n",
    "Como critério para classificação das mensagens, seguimos o seguinte padrão: todos os tweets que pudessem, de alguma forma, denegrir a imagem da empresa ou servir de base para incrementação e modificação de sua linha de produtos (como as mensagens que insinuam que o produto é para jovens e não para adultos, ou que explicitam nostalgia em relação a algum sabor não mais presente na linha, por exemplo) foram classificados como relevantes, enquanto os demais, como irrelevantes.\n",
    "\n",
    "Primeiramente, utilizamos o Classificador Naive Bayes de uma das bibliotecas do próprio Python para gerar os resultados: uma forma mais simples e rápida. Eis que este classificou todas as mensagens da nossa base de testes como irrelevantes. Fomos, então, averiguar se isso fazia algum sentido. Na base de testes, classificamos apenas 19,5% das mensagens como relevantes, o que é uma porcentagem pequena, enquanto os outros 80,5% deixamos como irrelevantes. Também percebemos que os tweets da base de testes eram bastante desconexos, isto é, distintos, em comparação à base de treinamento do classificador. Então, por meio de uma operação da mesma biblioteca do Python, dividimos a própria base de treinamento em algumas mensagens para treinar o classificador e outras para testá-lo, de modo que todas as mensagens da base de treinamento foram ora de treino, ora de testes. Feito isso, computamos a acurácia do modelo, isto é, a sua precisão, e obtivemos um valor de, aproximadamente, 68%.\n",
    "\n",
    "Partimos, então, para a construção do classificador 'na mão', isto é, realizando todos os cálculos por nós mesmas. Obtivemos que 7% dos tweets que eram relevantes verdadeiramente foram classificados como relevantes (positivos verdadeiros), contra 0% que eram irrelevantes, mas foram classificados como relevantes (positivos falsos). Obtivemos também que 80,5% dos irrelevantes foram classificados efetivamente como irrelevantes (negativos verdadeiros), contra 12,5% que eram relevantes, mas foram classificados como irrelevantes (negativos falsos). Percebemos que o classificador tem facilidade em classificar como relevantes mensagens com ampla repetição na base de testes e maior dificuldade com aquelas que não se repetem. Outro fator passível de destaque é que, pelo grande número de mensagens classificadas como irrelevantes na base de treinamento, muitas palavras que constavam nas mensagens relevantes também constavam nas irrelevantes, elevando a chance de o classificador se enganar ao classificar as primeiras na base de testes.\n",
    "\n",
    "De modo a saber como o classificador trata as mensagens com dupla negação e sarcasmo, analisamos, primeiramente, os resultados dados pelo classificador feito 'na mão' em relação à base de testes. Percebemos, então, que este tem dificuldade em identificar o sarcasmo e a dupla negação, classificando os tweets que contenham esse tipo de conteúdo como irrlevantes. Analisando os resultados obtidos com o classificador em relação à própria base de treinamento (operação supracitada), percebemos que, em alguns momentos, o classificador identificou ironias que possuíam certa relevância (como \"tá tão calor que vou abrir um pacote de ruffles pra ver se bate um ventinho\") efetivamente como relevantes e também identificou ironias que não possuíam relevância como irrelevantes (por exemplo, \"só tenho cara de idiota, dentro dessa moringa com formato de pacote de ruffles tem um cérebro\"). Em geral, as frases com dupla negação foram classificadas como irrelevantes (por exemplo, \"@emelyalissa não da não, o assassino é uma pessoa não o bicho papão e é uma serie muito fraca, assisti o filme que… https://t.co/npxs3i021t\"), embora o exemplo dado tenha sido realmente classificado por nós como irrelevante.\n",
    "\n",
    "Tendo adquirido uma ampla experiência nesse período de verdadeira imersão em análise de sentimentos para a execução do projeto, garantimos à empresa contratante que continuar financiando o nosso projeto é o melhor investimento que esta pode efetuar. Estamos cheias de insights para aperfeiçoar o classificador ao máximo. O primeiro passo é ampliar a base de treinamento: quanto mais habituado a novas \"situações\" (tipos de conteúdo), o classificador estiver, mais preciso ele será. O segundo passo é limpar ainda mais o texto da base de treino, reduzindo palavras de mesma família a um mesmo radical (para que todas as ocorrências de um mesmo tipo de palavra sejam classificadas como sendo a mesma palavra), criando uma lista das gírias e das abreviações para Internet mais populares em português para excluí-las da base de treinamento, separando as palavras escritas juntas, isto é, sem espaço (o que é muito comum no meio virtual), reduzindo expressões escritas de forma anormal a expressões regulares (\"churrascooooo\" - \"churrasco) e removendo as URL's dos tweets. Feito isso e incrementando outras ideias que forem surgindo em meio à empreitada, construiremos o melhor classificador já visto, capaz de identificar quase que com precisão de 100% ironias, sarcasmo e classificando as mensagens, com êxito, em relevantes e irrelevantes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "É perceptível que não podemos alimentar a base de Treinamento automaticamente utilizando o próprio classificador porque, embora treinemos este com o maior número possível de tweets, ainda assim ele não possuirá uma inteligência humana, isto é, há possibilidade de que surjam tipos de conteúdo, sentimentalismo, com os quais ele não terá familiaridade e classificará de forma errada. Quanto mais classificações erradas ele fizer na base de treinamento, maior a propagação do erro e menor a sua precisão nos testes. A base de treinamento é uma referência. Esta tem de estar de acordo com o que nós, humanos, consideramos como relevante, para que o classificador aprenda a olhar as mensagens com os nossos olhos e erre cada vez menos em suas classificações, nos mostrando apenas aquilo que realmente queremos ver. \n",
    "\n",
    "Embora o classificador Naive Bayes seja muito comum em classificação de textos, uso o qual demonstramos nesse projeto, este também pode ser utilizado em diagnóstico médico - para detectar a probabilidade de um diagnóstico estar certo ou errado para determinada doença ou para determinar a probabilidade de um indivíduo continuar a ter uma doença após o tratamento, como câncer, por exemplo-, Engenharia de Controle - para identificar possíveis falhas na movimentação e execução de tarefas de robôs, por exemplo.\n",
    "\n",
    "Para tornar o classificador cada vez mais preciso, em categorização de textos, ao menos, é possível implementar todas as soluções descritas anteriormente (em nossa defesa para a continuação do financiamento de nosso projeto pela empresa)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
