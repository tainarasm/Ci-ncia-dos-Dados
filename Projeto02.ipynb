{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Classificador Automático de Sentimento\n",
    "\n",
    "Você foi contratado por uma empresa parar analisar como os clientes estão reagindo a um determinado produto no Twitter. A empresa deseja que você crie um programa que irá analisar as mensagens disponíveis e classificará como \"relevante\" ou \"irrelevante\". Com isso ela deseja que mensagens negativas, que denigrem o nome do produto, ou que mereçam destaque, disparem um foco de atenção da área de marketing.<br /><br />\n",
    "Como aluno de Ciência dos Dados, você lembrou do Teorema de Bayes, mais especificamente do Classificador Naive-Bayes, que é largamente utilizado em filtros anti-spam de e-mails. O classificador permite calcular qual a probabilidade de uma mensagem ser relevante dadas as palavras em seu conteúdo.<br /><br />\n",
    "Para realizar o MVP (*minimum viable product*) do projeto, você precisa implementar uma versão do classificador que \"aprende\" o que é relevante com uma base de treinamento e compara a performance dos resultados com uma base de testes.<br /><br />\n",
    "Após validado, o seu protótipo poderá também capturar e classificar automaticamente as mensagens da plataforma.\n",
    "\n",
    "## Informações do Projeto\n",
    "\n",
    "Prazo: 13/Set até às 23:59.<br />\n",
    "Grupo: 1 ou 2 pessoas.<br /><br />\n",
    "Entregáveis via GitHub: \n",
    "* Arquivo notebook com o código do classificador, seguindo as orientações abaixo.\n",
    "* Arquivo Excel com as bases de treinamento e teste totalmente classificado.\n",
    "\n",
    "**NÃO disponibilizar o arquivo com os *access keys/tokens* do Twitter.**\n",
    "\n",
    "\n",
    "### Check 3: \n",
    "\n",
    "Até o dia 06 de Setembro às 23:59, o notebook e o xlsx devem estar no Github com as seguintes evidências: \n",
    "    * Conta no twitter criada.\n",
    "    * Produto escolhido.\n",
    "    * Arquivo Excel contendo a base de treinamento e teste já classificado.\n",
    "\n",
    "Sugestão de leitura:<br />\n",
    "http://docs.tweepy.org/en/v3.5.0/index.html<br />\n",
    "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Preparando o ambiente\n",
    "\n",
    "Instalando a biblioteca *tweepy* para realizar a conexão com o Twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy\n",
    "#!pip install -U textblob\n",
    "!pip install sklearn-pandas\n",
    "\n",
    "#python -m textblob.download_corpora\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as Bibliotecas que serão utilizadas. Esteja livre para adicionar outras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "from random import shuffle\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "#import nltk\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import string\n",
    "\n",
    "from nltk.tokenize.moses import MosesTokenizer, MosesDetokenizer\n",
    "\n",
    "\n",
    "#from textblob import TextBlob\n",
    "#from textblob.classifiers import NaiveBayesClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Autenticando no  Twitter\n",
    "\n",
    "Para realizar a captura dos dados é necessário ter uma conta cadastrada no twitter:\n",
    "\n",
    "* Conta: ***@tainarasm98***\n",
    "\n",
    "\n",
    "1. Caso ainda não tenha uma: https://twitter.com/signup\n",
    "1. Depois é necessário registrar um app para usar a biblioteca: https://apps.twitter.com/\n",
    "1. Dentro do registro do App, na aba Keys and Access Tokens, anotar os seguintes campos:\n",
    "    1. Consumer Key (API Key)\n",
    "    1. Consumer Secret (API Secret)\n",
    "1. Mais abaixo, gere um Token e anote também:\n",
    "    1. Access Token\n",
    "    1. Access Token Secret\n",
    "    \n",
    "1. Preencha os valores no arquivo \"auth.pass\"\n",
    "\n",
    "**ATENÇÃO**: Nunca divulgue os dados desse arquivo online (GitHub, etc). Ele contém as chaves necessárias para realizar as operações no twitter de forma automática e portanto é equivalente a ser \"hackeado\". De posse desses dados, pessoas mal intencionadas podem fazer todas as operações manuais (tweetar, seguir, bloquear/desbloquear, listar os seguidores, etc). Para efeito do projeto, esse arquivo não precisa ser entregue!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dados de autenticação do twitter:\n",
    "\n",
    "#Coloque aqui o identificador da conta no twitter: @tainarasm98\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "with open('auth.pass') as fp:    \n",
    "    data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. Não modificar\n",
    "auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "auth.set_access_token(data['access_token'], data['access_token_secret'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Coletando Dados\n",
    "\n",
    "Agora vamos coletar os dados. Tenha em mente que dependendo do produto escolhido, não haverá uma quantidade significativa de mensagens, ou ainda poder haver muitos retweets.<br /><br /> \n",
    "Configurando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Produto escolhido:\n",
    "produto = \"Ruffles\"\n",
    "\n",
    "#Quantidade mínima de mensagens capturadas:\n",
    "n = 500\n",
    "#Quantidade mínima de mensagens para a base de treinamento:\n",
    "t = 300\n",
    "\n",
    "#Filtro de língua, escolha uma na tabela ISO 639-1.\n",
    "lang = 'pt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturando os dados do twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cria um objeto para a captura\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#Inicia a captura, para mais detalhes: ver a documentação do tweepy\n",
    "i = 1\n",
    "msgs = []\n",
    "for msg in tweepy.Cursor(api.search, q=produto, lang=lang).items():    \n",
    "    msgs.append(msg.text.lower())\n",
    "    i += 1\n",
    "    if i > n:\n",
    "        break\n",
    "\n",
    "#Embaralhando as mensagens para reduzir um possível viés\n",
    "shuffle(msgs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando os dados em uma planilha Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Verifica se o arquivo não existe para não substituir um conjunto pronto\n",
    "if not os.path.isfile('./{0}.xlsx'.format(produto)):\n",
    "    \n",
    "    #Abre o arquivo para escrita\n",
    "    writer = pd.ExcelWriter('{0}.xlsx'.format(produto))\n",
    "    \n",
    "    #divide o conjunto de mensagens em duas planilhas\n",
    "    dft = pd.DataFrame({'Treinamento' : pd.Series(msgs[:t])})\n",
    "    dft.to_excel(excel_writer = writer, sheet_name = 'Treinamento', index = False)\n",
    "\n",
    "    dfc = pd.DataFrame({'Teste' : pd.Series(msgs[t:])})\n",
    "    dfc.to_excel(excel_writer = writer, sheet_name = 'Teste', index = False)\n",
    "\n",
    "    #fecha o arquivo\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Classificando as Mensagens\n",
    "\n",
    "Agora você deve abrir o arquivo Excel com as mensagens capturadas e classificar na Coluna B se a mensagem é relevante ou não.<br /> \n",
    "Não se esqueça de colocar um nome para a coluna na célula **B1**.<br /><br />\n",
    "Fazer o mesmo na planilha de Controle.\n",
    "\n",
    "___\n",
    "## Montando o Classificador Naive-Bayes\n",
    "\n",
    "Com a base de treinamento montada, comece a desenvolver o classificador. Escreva o seu código abaixo:\n",
    "\n",
    "Opcionalmente: \n",
    "* Limpar as mensagens removendo os caracteres: enter, :, \", ', (, ), etc. Não remover emojis.<br />\n",
    "* Corrigir separação de espaços entre palavras e/ou emojis.\n",
    "* Propor outras limpezas/transformações que não afetem a qualidade da informação.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-a15cf7e8fad9>, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-a15cf7e8fad9>\"\u001b[1;36m, line \u001b[1;32m35\u001b[0m\n\u001b[1;33m    \"\"\"for i in range(len(relevante['Treinamento'])):\u001b[0m\n\u001b[1;37m                                                     \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"xls_file = pd.ExcelFile('Ruffles.xlsx')\n",
    "\n",
    "df = xls_file.parse('Treinamento')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "relevante = pd.DataFrame({'Treinamento': df['Treinamento'],\n",
    "                          'Classificação': df['Classificação']})\n",
    "\n",
    "irrelevante = pd.DataFrame({'Treinamento': df['Treinamento'],\n",
    "                          'Classificação': df['Classificação']})\n",
    "\n",
    "relevante = relevante[(df['Classificação']=='Relevante')]\n",
    "\n",
    "relevante['Treinamento'].apply(str)\n",
    "#relevante['Treinamento'] = relevante['Treinamento'].astype('category')\n",
    "print(relevante.dtypes)\n",
    "#relevante = [w for w in relevante['Treinamento'] if not w in stopwords.words(\"portuguese\")]\n",
    "\n",
    "\n",
    "treino = df['Treinamento']\n",
    "irrelevante = irrelevante[(df['Classificação']=='Irrelevante')]\n",
    "\n",
    "\n",
    "\n",
    "df['Treinamento'] = df['Treinamento'].str.strip()\n",
    "df['Treinamento'] = df['Treinamento'].str.lower().str.split()\n",
    "\n",
    "count_vect = CountVectorizer(analyzer=\"word\")\n",
    "freq_tweets = count_vect.fit_transform(treino)\n",
    "modelo = MultinomialNB()\n",
    "modelo.fit(freq_tweets.treino)\n",
    "\n",
    "\"\"\"for i in range(len(relevante['Treinamento'])):\n",
    "    treinamento_counts = count_vect.fit_transform(df['Treinamento'][i])\n",
    "    treinamento_counts.shape \n",
    "    count_vect.vocabulary_.get(u'algorithm')\n",
    "    #print(count_vect.vocabulary_)\"\"\"\n",
    "\n",
    "\n",
    "#a = Counter(df['Treinamento'].str.split())\n",
    "#print(a)\n",
    "\n",
    "\n",
    "#d['mynewkey'] = 'mynewvalue'\n",
    "\n",
    "\n",
    "#a = Counter(df['Treinamento'][0])\n",
    "#print(a)\n",
    "\n",
    "\n",
    "relevante = df['Classificação'].value_counts().reindex(['Relevante', 'Irrelevante'])\n",
    "\n",
    "treinamento = df['Treinamento']\n",
    "\n",
    "\n",
    "print()\n",
    "#stop words - para limpar\n",
    "\n",
    "#print (stopwords.words(\"portuguese\")) \n",
    "\n",
    "#for i in range(len(df['Treinamento'])):\n",
    "    #if df['Treinamento'][i] in stopwords.words('portuguese'):\n",
    "        #print('sim')\n",
    "        #df['Treinamento'][i] in df['Treinamento'][i]\"\"\"\n",
    "        \n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-b159addf8395>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mtreinamento\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtreinamento\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mfrase\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtreinamento\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpalavra\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfrase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "xls_file = pd.ExcelFile('Ruffles.xlsx')\n",
    "\n",
    "df = xls_file.parse('Treinamento')\n",
    "df2 = xls_file.parse('Teste')\n",
    "\n",
    "\n",
    "treinamento = pd.DataFrame({'Treinamento': df['Treinamento']})\n",
    "treinamento = treinamento['Treinamento']\n",
    "stop = stopwords.words('portuguese')\n",
    "#stops = str(stops)\n",
    "remove_list = []\n",
    "\n",
    "\"\"\"for line in treinamento['Treinamento']:\n",
    "    for w in line.split():\n",
    "        if w.lower() in stops:\n",
    "            if w.lower() not in remove_list:\n",
    "                remove_list.append(w)\"\"\"\n",
    "      \n",
    "treinamento = list(treinamento)   \n",
    "\"\"\"\"\"\"for frase in treinamento.split():\n",
    "    print(frase)\n",
    "    for palavra in frase:\n",
    "        if palavra.lower() in stopwords.words('portuguese'):\n",
    "            treinamento = treinamento.remove(palavra)\n",
    "    \n",
    "\n",
    "print(treinamento)\n",
    "#print(treinamento['Treinamento'])\n",
    "#for i in treinamento['Treinamento'].str.split():\n",
    "    #for j in i:\n",
    "        #if j in remove_list:\n",
    "            #treinamento['Treinamento'] = treinamento['Treinamento'].str.split(j)\n",
    "\n",
    "\n",
    "#print(treinamento['Treinamento'])    \n",
    "\n",
    "remove_list = str(remove_list)\n",
    "#remove_list.split()\n",
    "\n",
    "#['Treinamento'] = treinamento['Treinamento'].apply(lambda x: [item for item in string.split(x) if item not in stop])\n",
    "#treinamento['Treinamento'] = treinamento['Treinamento'].str.strip().str.split()\n",
    "df['Treinamento'] = df['Treinamento'].str.lower()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#treinamento['Treinamento'] = treinamento['Treinamento'].str.strip().str.lower().str.split()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"for i in range(len(treinamento['Treinamento'])):\n",
    "    if treinamento['Treinamento'][i] not in stops:\n",
    "        treinamento['Treinamento'][i]in treinamento['Treinamento'][i]\"\"\"\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "#print(treinamento['Treinamento'])\n",
    "\n",
    "\n",
    "#filteredtext = [t for t in treinamento['Treinamento'] if t not in stopwords.words('portuguese')]\n",
    "#print(filteredtext)\n",
    "\n",
    "\n",
    "classificacao = pd.DataFrame({'Classificação': df['Classificação']})\n",
    "teste = pd.DataFrame({'Teste': df2['Teste']})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"vectorizer = CountVectorizer(analyzer=\"word\")\n",
    "freq_tweets = vectorizer.fit_transform(treinamento['Treinamento'])\n",
    "modelo = MultinomialNB(alpha=1.0)\n",
    "modelo.fit(freq_tweets,classificacao['Classificação'])\n",
    "\n",
    "\n",
    "freq_testes = vectorizer.transform(teste['Teste'])\n",
    "modelo.predict(freq_testes)\n",
    "\n",
    "resultados = cross_val_predict(modelo, freq_tweets, classificacao['Classificação'], cv=10)\n",
    "print(metrics.accuracy_score(classificacao['Classificação'],resultados))\n",
    "\n",
    "sentimento=['Relevante','Irrelevante']\n",
    "print(metrics.classification_report(classificacao['Classificação'],resultados,sentimento))\n",
    "\n",
    "pd.crosstab(classificacao['Classificação'], resultados, rownames=['Real'], colnames=['Predito'], margins=True)\"\"\"\n",
    "\n",
    "treinamento['Treinamento']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.676666666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  Relevante       0.30      0.62      0.40        53\n",
      "Irrelevante       0.89      0.69      0.78       247\n",
      "\n",
      "avg / total       0.79      0.68      0.71       300\n",
      "\n",
      "0      [fvernon, quer, saber, vou, abrir, ruffles, co...\n",
      "1      [oferecimento, rufflesoficial, motorola, adida...\n",
      "2      [rt, doreajheniffer, memarialuiza, hunterrowla...\n",
      "3                       [10, conto, ruffles, quebrei, 💔]\n",
      "4      [emelyalissa, assassino, é, pessoa, bicho, pap...\n",
      "5      [rt, loveologyx, vida, adulta, fez, desgostar,...\n",
      "6                         [104, ruffles, miojo, doritos]\n",
      "7      [rt, larissakohnlein, vida, igual, pacote, ruf...\n",
      "8      [fecroft, senti, lá, belíssima, tomando, vinho...\n",
      "9      [vontade, insana, comer, ruffles, logo, hoje, ...\n",
      "10                          [ainda, bem, gosto, ruffles]\n",
      "11                  [to, comendo, ruffles, bem, buenass]\n",
      "12       [fuckusofmuch, nevermindalot, nao, vai, chegou]\n",
      "13     [to, aqui, salão, arrumando, cabelo, mãe, trou...\n",
      "14     [chrisandre41, rufflesoficial, motorola, adida...\n",
      "15     [galaxydraw, mano, ja, criamos, grupo, team, c...\n",
      "16     [batata, ruffles, churrasco, então, horrível, ...\n",
      "17     [rt, morais2904, loiranoob, vejo, alguem, abri...\n",
      "18         [verdadeiro, amor, é, ruffles, churrasco, ♥️]\n",
      "19                       [habitantes, chamarem, ruffles]\n",
      "20     [ultimamente, sentido, vazio, embalagem, ruffles]\n",
      "21     [kccprobiem, fodase, cada, gosto, hein, pego, ...\n",
      "22     [alguem, ja, percebeu, ruffles, agora, explica...\n",
      "23                       [to, vontade, comer, ruffles😑❤]\n",
      "24     [bah, salgadão, cara, pagava, 50, ta, preço, r...\n",
      "25        [tô, desde, ontem, dormir, ainda, tô, agitada]\n",
      "26     [rt, kccstomlinson, amor, deu, ruffles, aaaaaa...\n",
      "27     [nnanzin, rufflesoficial, motorola, adidas, ag...\n",
      "28                       [comendo, ruffles, café, manhã]\n",
      "29     [ta, certo, q, ruffles, é, metade, ar, mano, b...\n",
      "                             ...                        \n",
      "270    [viniciusrlima, amava, ruffles, aí, comprei, p...\n",
      "271    [posso, comer, cheetos, ruffles, doritos, baco...\n",
      "272       [to, brocaaaa, ruffles, marcando, aq, kkkkkkk]\n",
      "273    [sacanagem, é, comprar, montão, vir, 50, bacon...\n",
      "274                             [italo, deu, 5, ruffles]\n",
      "275    [trupebae, rahsampaio, mariaedrrr, sim, vou, e...\n",
      "276    [rt, morais2904, loiranoob, vejo, alguem, abri...\n",
      "277               [qebrei, dente, comendo, ruffles, tnc]\n",
      "278    [equalstewart, vdd, viu, auabauhaha, n, posso,...\n",
      "279        [rt, fatosbrisa, ｃｕｉｄａ, ｄａ, ｓｕａ, ｖｉｄａ, ｐｏｒｒａ]\n",
      "280    [rt, morais2904, loiranoob, vejo, alguem, abri...\n",
      "281    [rt, morais2904, loiranoob, vejo, alguem, abri...\n",
      "282                             [desejo, comer, ruffles]\n",
      "283                            [tô, machucando, ruffles]\n",
      "284    [dia, lindo, parece, inverno, hj, desce, mille...\n",
      "285    [tá, tão, calor, vou, abrir, pacote, ruffles, ...\n",
      "286    [rt, daniijubz, promoções, pingo, doce, ruffle...\n",
      "287                              [rafa31ruffles, ligame]\n",
      "288    [cara, idiota, dentro, dessa, moringa, formato...\n",
      "289    [alez2001, ir, ni, subway, comer, ruffles, gra...\n",
      "290    [bem, bomm, shé, ruffles, doutros, sabores, pr...\n",
      "291    [insano, ir, academia, ver, todos, exercitando...\n",
      "292               [ta, rolando, ruffles, d, churrascooo]\n",
      "293      [acabei, comer, ruffles, ainda, quero, ruffles]\n",
      "294    [comprei, dois, patocão, ruffles, to, olhando,...\n",
      "295    [vou, comprar, ruffles, guaraná, pra, levar, v...\n",
      "296    [rt, yuditamashiro, amelhorprofissãodomundo, é...\n",
      "297    [rt, morais2904, loiranoob, vejo, alguem, abri...\n",
      "298          [viniciusrlima, amo, fini, ruffles, cebola]\n",
      "299                  [batata, palha, é, melhor, ruffles]\n",
      "Name: Tokenized_treinamento, Length: 300, dtype: object\n"
     ]
    }
   ],
   "source": [
    "xls_file = pd.ExcelFile('Ruffles.xlsx')\n",
    "\n",
    "df = xls_file.parse('Treinamento')\n",
    "df2 = xls_file.parse('Teste')\n",
    "\n",
    "\n",
    "treinamento = pd.DataFrame({'Treinamento': df['Treinamento']})\n",
    "classificacao = pd.DataFrame({'Classificação': df['Classificação']})\n",
    "teste = pd.DataFrame({'Teste': df2['Teste']})\n",
    "                            \n",
    "treinamento['Tokenized_treinamento'] = treinamento.apply(lambda row: nltk.word_tokenize(row['Treinamento']), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "stop = stopwords.words('portuguese')\n",
    "\n",
    "#treinamento = \" \".join(re.findall('[A-Z][^A-Z]*', treinamento)) - importante vooos\n",
    "\n",
    "#tokenized_treinamento = [word_tokenize(palavra) for palavra in treinamento['Treinamento']]\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "tokenized_treinamento_sem_pontuacao = []\n",
    "\n",
    "for review in treinamento['Tokenized_treinamento']:\n",
    "    new_review = []\n",
    "    for token in review: \n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "\n",
    "    tokenized_treinamento_sem_pontuacao.append(new_review)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "tokenized_treinamento_sem_stopwords = []\n",
    "for frase in tokenized_treinamento_sem_pontuacao:\n",
    "    new_term_vector = []\n",
    "    for palavra in frase:\n",
    "        if not palavra in stopwords.words('portuguese'):\n",
    "            new_term_vector.append(palavra)\n",
    "    tokenized_treinamento_sem_stopwords.append(new_term_vector)\n",
    "\n",
    "    \n",
    "treinamento['Tokenized_treinamento'] = tokenized_treinamento_sem_stopwords\n",
    "\n",
    "\n",
    "\n",
    "#\"\".join(treinamento['Tokenized_treinamento'])\n",
    "\n",
    "#d = MosesDetokenizer()\n",
    "#detokens = d.detokenize(tokenized_treinamento_sem_stopwords)\n",
    "#detokens = \"\".join(detokens)\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', tokenizer=lambda doc: doc, lowercase=False)\n",
    "freq_tweets = vectorizer.fit_transform(treinamento['Tokenized_treinamento'])\n",
    "\n",
    "#freq_tweets = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False).fit_transform(treinamento['Tokenized_treinamento'])\n",
    "freq_tweets.toarray()\n",
    "modelo = MultinomialNB(alpha=1)\n",
    "modelo.fit(freq_tweets,classificacao['Classificação'])\n",
    "\n",
    "\n",
    "freq_testes = vectorizer.transform(teste['Teste'])\n",
    "modelo.predict(freq_testes)\n",
    "\n",
    "\n",
    "resultados = cross_val_predict(modelo, freq_tweets, classificacao['Classificação'], cv=10)\n",
    "print(metrics.accuracy_score(classificacao['Classificação'],resultados))\n",
    "\n",
    "sentimento=['Relevante','Irrelevante']\n",
    "print(metrics.classification_report(classificacao['Classificação'],resultados,sentimento))\n",
    "\n",
    "pd.crosstab(classificacao['Classificação'], resultados, rownames=['Real'], colnames=['Predito'], margins=True)\n",
    "\n",
    "\n",
    "teste['Teste'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taina\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-380-6caa12fc6be9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq_testes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mresultados\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq_tweets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassificacao\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m#metrics.accuracy_score(classificacao,resultados)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\taina\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_predict\u001b[1;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, pre_dispatch, method)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m     \u001b[0mcv_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[1;31m# Ensure the estimator has implemented the passed decision function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\taina\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    320\u001b[0m                                                              n_samples))\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_BaseKFold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\taina\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter_test_masks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m             \u001b[0mtrain_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[0mtest_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\taina\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_iter_test_masks\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_iter_test_masks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m         \u001b[0mtest_folds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_folds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mtest_folds\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\taina\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_make_test_folds\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    595\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtest_fold_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mper_cls_splits\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mper_cls_cvs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_split\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mper_cls_splits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m                 \u001b[0mcls_test_folds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_folds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m                 \u001b[1;31m# the test split can be too big because we used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m                 \u001b[1;31m# KFold(...).split(X[:max(c, n_splits)]) when data is not 100%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,2), analyzer='word', tokenizer=lambda doc: doc, lowercase=False)\n",
    "freq_tweets = vectorizer.fit_transform(treinamento['Tokenized_treinamento'])\n",
    "modelo = MultinomialNB()\n",
    "modelo.fit(freq_tweets,classificacao)\n",
    "\n",
    "freq_testes = vectorizer.transform(teste['Teste'])\n",
    "modelo.predict(freq_testes)\n",
    "\n",
    "resultados = cross_val_predict(modelo, freq_tweets, classificacao, cv=10)\n",
    "#metrics.accuracy_score(classificacao,resultados)\n",
    "\n",
    "#sentimento=['Relevante','Irrelevante']\n",
    "#print(metrics.classification_report(classificacao,resultados,sentimento))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Irrelevante    0.86\n",
       "Relevante      0.14\n",
       "Name: Classificação, dtype: float64"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xls_file = pd.ExcelFile('Ruffles.xlsx')\n",
    "\n",
    "df = xls_file.parse('Treinamento')\n",
    "df2 = xls_file.parse('Teste')\n",
    "\n",
    "treinamento = pd.DataFrame({'Treinamento': df['Treinamento'],\n",
    "                            'Classificação': df['Classificação']})\n",
    "\n",
    "\n",
    "teste = pd.DataFrame({'Teste': df2['Teste']})\n",
    "                            \n",
    "treinamento['Tokenized_treinamento'] = treinamento.apply(lambda row: nltk.word_tokenize(row['Treinamento']), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "stop = stopwords.words('portuguese')\n",
    "\n",
    "#treinamento = \" \".join(re.findall('[A-Z][^A-Z]*', treinamento)) - importante vooos\n",
    "\n",
    "#tokenized_treinamento = [word_tokenize(palavra) for palavra in treinamento['Treinamento']]\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "tokenized_treinamento_sem_pontuacao = []\n",
    "\n",
    "for review in treinamento['Tokenized_treinamento']:\n",
    "    new_review = []\n",
    "    for token in review: \n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "\n",
    "    tokenized_treinamento_sem_pontuacao.append(new_review)\n",
    "        \n",
    "\n",
    "\n",
    "tokenized_treinamento_sem_stopwords = []\n",
    "for frase in tokenized_treinamento_sem_pontuacao:\n",
    "    new_term_vector = []\n",
    "    for palavra in frase:\n",
    "        if not palavra in stopwords.words('portuguese'):\n",
    "            new_term_vector.append(palavra)\n",
    "    tokenized_treinamento_sem_stopwords.append(new_term_vector)\n",
    "\n",
    "    \n",
    "treinamento['Tokenized_treinamento'] = tokenized_treinamento_sem_stopwords\n",
    "relevante = treinamento[(treinamento.Classificação == 'Relevante')]\n",
    "irrelevante = treinamento[(treinamento.Classificação ==  'Irrelevante')]\n",
    "\n",
    "\n",
    "\n",
    "all_words = []\n",
    "all_words2 = []\n",
    "frequencia = {}\n",
    "frequencia2 = {}\n",
    "prob_palavra_relevante = {}\n",
    "prob_palavra_irrelevante = {}\n",
    "\n",
    "\n",
    "for percorre_tweets in relevante['Tokenized_treinamento']:\n",
    "    all_words.extend(percorre_tweets)\n",
    "\n",
    "for percorre_tweets in irrelevante['Tokenized_treinamento']:\n",
    "    all_words2.extend(percorre_tweets)\n",
    "\n",
    "for i in all_words:\n",
    "    frequencia[i] = all_words.count(i)\n",
    "\n",
    "for i in all_words2:\n",
    "    frequencia2[i] = all_words2.count(i)   \n",
    "\n",
    "for j in frequencia:\n",
    "    prob_palavra_relevante[j] = ((frequencia[j]+1)/(len(all_words) + (len(frequencia)+len(frequencia2))))\n",
    "\n",
    "for j in frequencia2:\n",
    "    prob_palavra_irrelevante[j] = ((frequencia2[j]+1)/(len(all_words) + (len(frequencia)+len(frequencia2))))\n",
    "\n",
    "prob_rel_irr = df.Classificação.value_counts(True)\n",
    "\n",
    "prob_teste = df2.Classificação.value_counts(True)\n",
    "\n",
    "#wordlist = nltk.FreqDist(wordlist)\n",
    "#word_features = all_words.keys()\n",
    "\n",
    "#print(all_words)\n",
    "\n",
    "prob_teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Verificando a performance\n",
    "\n",
    "Agora você deve testar o seu Classificador com a base de Testes.<br /><br /> \n",
    "\n",
    "Você deve extrair as seguintes medidas:\n",
    "* Porcentagem de positivos falsos (marcados como relevante mas não são relevantes)\n",
    "* Porcentagem de positivos verdadeiros (marcado como relevante e são relevantes)\n",
    "* Porcentagem de negativos verdadeiros (marcado como não relevante e não são relevantes)\n",
    "* Porcentagem de negativos falsos (marcado como não relevante e são relevantes)\n",
    "\n",
    "Opcionalmente:\n",
    "* Criar categorias intermediárias de relevância baseado na diferença de probabilidades. Exemplo: muito relevante, relevante, neutro, irrelevante e muito irrelevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2195337805982807e-65]\n",
      "[2.6142246197088749e-47]\n",
      "[9.9426833603149106e-81]\n",
      "[1.6761462901521062e-90]\n",
      "[1.4876590664058873e-63]\n",
      "[9.9426833603149106e-81]\n",
      "[1.846019546928247e-33]\n",
      "[1.2195337805982807e-65]\n",
      "[4.9246871156242447e-76]\n",
      "[1.2171663356499402e-38]\n",
      "[1.4876590664058873e-63]\n",
      "[1.4876590664058873e-63]\n",
      "[2.6362992198278426e-38]\n",
      "[1.4876590664058873e-63]\n",
      "[2.9912150898816006e-32]\n",
      "[2.3144145207962853e-33]\n",
      "[5.9115654556112042e-20]\n",
      "[1.2195337805982807e-65]\n",
      "[1.3788109024871128e-39]\n",
      "[1.1476964928203583e-86]\n",
      "[1.1624949553858039e-32]\n",
      "[1.304632629681158e-32]\n",
      "[1.6148439573838434e-37]\n",
      "[1.2195337805982807e-65]\n",
      "[8.9708089431401581e-12]\n",
      "[9.9145706809560798e-54]\n",
      "[1.4210633941830906e-39]\n",
      "[2.1792814233979742e-84]\n",
      "[7.692383507433781e-19]\n",
      "[3.8800412353123243e-67]\n",
      "[2.2860327024561474e-55]\n",
      "[1.7207258992838778e-46]\n",
      "[1.2195337805982807e-65]\n",
      "[3.5411751542870352e-51]\n",
      "[4.7921995361493059e-32]\n",
      "[1.2195337805982807e-65]\n",
      "[9.0037766427087171e-58]\n",
      "[5.418408770803415e-56]\n",
      "[1.4876590664058873e-63]\n",
      "[2.9539916006198276e-24]\n",
      "[5.9211427168289359e-14]\n",
      "[1.6913204355013903e-54]\n",
      "[1.4876590664058873e-63]\n",
      "[5.8092545692963673e-31]\n",
      "[1.8803639790090875e-28]\n",
      "[1.7403466255621344e-42]\n",
      "[2.4709333197741834e-25]\n",
      "[3.8499011464174043e-19]\n",
      "[1.9004216598216968e-39]\n",
      "[3.6419755686696352e-17]\n",
      "[1.1302967920022452e-17]\n",
      "[1.7675911981668134e-46]\n",
      "[1.4876590664058873e-63]\n",
      "[1.8399031314160015e-36]\n",
      "[3.8352790159361551e-47]\n",
      "[3.4035597771755381e-58]\n",
      "[2.3040969938594783e-52]\n",
      "[9.0797123289144746e-81]\n",
      "[2.5093859207024622e-34]\n",
      "[1.2195337805982807e-65]\n",
      "[1.2195337805982807e-65]\n",
      "[1.4876590664058873e-63]\n",
      "[2.671379402544889e-22]\n",
      "[4.0357371750677641e-58]\n",
      "[3.916056185831188e-56]\n",
      "[1.2195337805982807e-65]\n",
      "[9.9426833603149106e-81]\n",
      "[1.4876590664058873e-63]\n",
      "[9.9426833603149106e-81]\n",
      "[1.2253558484213448e-39]\n",
      "[2.1708748297520151e-49]\n",
      "[2.2799829754161373e-67]\n",
      "[5.3444480405314006e-44]\n",
      "[1.4672042218302445e-88]\n",
      "[7.083701265252965e-88]\n",
      "[2.8857580836872681e-48]\n",
      "[1.4876590664058873e-63]\n",
      "[1.2195337805982807e-65]\n",
      "[1.0163506057216219e-95]\n",
      "[1.4876590664058873e-63]\n",
      "[1.4876590664058873e-63]\n",
      "[4.0455794359786894e-22]\n",
      "[8.9780358120310145e-62]\n",
      "[9.9426833603149106e-81]\n",
      "[1.4876590664058873e-63]\n",
      "[9.9426833603149106e-81]\n",
      "[2.5938132178020048e-25]\n",
      "[2.8490363318319519e-29]\n",
      "[1.2195337805982807e-65]\n",
      "[1.2079261557216273e-43]\n",
      "[2.6297084717782737e-36]\n",
      "[9.9426833603149106e-81]\n",
      "[1.2195337805982807e-65]\n",
      "[7.9837610827562878e-25]\n",
      "[6.7171965665694506e-72]\n",
      "[8.7040132893714308e-37]\n",
      "[4.9381540693825065e-53]\n",
      "[5.9259371643729282e-10]\n",
      "[1.56911738179772e-35]\n",
      "[9.8512267211391503e-58]\n",
      "[2.0450286516007623e-50]\n",
      "[1.2195337805982807e-65]\n",
      "[2.0075135491645246e-45]\n",
      "[1.1737723457858144e-61]\n",
      "[2.5821266123234842e-26]\n",
      "[1.4876590664058873e-63]\n",
      "[9.9426833603149106e-81]\n",
      "[2.4635330539078597e-22]\n",
      "[3.1574750504282531e-52]\n",
      "[1.0030996929988075e-53]\n",
      "[8.0828202412573859e-30]\n",
      "[1.4750112764964856e-53]\n",
      "[5.4834064357398343e-45]\n",
      "[9.5674019873707647e-18]\n",
      "[1.9216502320898782e-22]\n",
      "[7.3023846225597428e-45]\n",
      "[5.4829859504691604e-68]\n",
      "[2.6457960715899206e-61]\n",
      "[1.5034593333619048e-40]\n",
      "[1.2195337805982807e-65]\n",
      "[2.1877113942371947e-64]\n",
      "[3.2939470784957093e-42]\n",
      "[9.6869021266974389e-34]\n",
      "[1.2195337805982807e-65]\n",
      "[2.0100846450543957e-22]\n",
      "[2.1064019504046774e-35]\n",
      "[1.0353616259503672e-15]\n",
      "[7.8569678569141164e-38]\n",
      "[1.4876590664058873e-63]\n",
      "[1.2482447442426279e-28]\n",
      "[1.2195337805982807e-65]\n",
      "[7.4205812005820354e-26]\n",
      "[8.16377639152386e-62]\n",
      "[1.9829012757857422e-93]\n",
      "[3.5546488440383874e-50]\n",
      "[1.8186896324684011e-64]\n",
      "[1.2195337805982807e-65]\n",
      "[1.9426133871613657e-80]\n",
      "[7.9095311064250132e-34]\n",
      "[6.1217755265944168e-70]\n",
      "[1.0330476609503866e-60]\n",
      "[9.9554983696130395e-36]\n",
      "[7.5603630018052982e-59]\n",
      "[6.1384762030623144e-87]\n",
      "[1.2195337805982807e-65]\n",
      "[3.8052082381669472e-49]\n",
      "[6.7930971213358974e-41]\n",
      "[2.3879817542540494e-24]\n",
      "[1.2627705696779218e-29]\n",
      "[3.5999724938503254e-26]\n",
      "[1.0189073907777403e-37]\n",
      "[9.9426833603149106e-81]\n",
      "[4.5647137675924315e-45]\n",
      "[9.9426833603149106e-81]\n",
      "[8.797079720891764e-28]\n",
      "[6.1384762030623144e-87]\n",
      "[2.0392850103285328e-103]\n",
      "[7.3834782945791242e-24]\n",
      "[7.5010268227156796e-45]\n",
      "[1.4876590664058873e-63]\n",
      "[2.278512923389892e-19]\n",
      "[9.9426833603149106e-81]\n",
      "[1.2195337805982807e-65]\n",
      "[2.4959563290078655e-07]\n",
      "[3.6576523239298365e-55]\n",
      "[2.9581760741439067e-16]\n",
      "[2.1589107008321953e-23]\n",
      "[2.6457960715899206e-61]\n",
      "[5.6934625725120701e-33]\n",
      "[1.4963507350183034e-36]\n",
      "[1.9262114630255466e-13]\n",
      "[2.6457960715899206e-61]\n",
      "[1.2195337805982807e-65]\n",
      "[8.9263458042647283e-16]\n",
      "[1.2195337805982807e-65]\n",
      "[1.4766956589158248e-23]\n",
      "[7.7683616972559663e-24]\n",
      "[4.8855257383224183e-53]\n",
      "[1.4790880370719534e-16]\n",
      "[1.1367654964009487e-26]\n",
      "[1.0573517337276248e-45]\n",
      "[1.4552191540165127e-51]\n",
      "[2.0683978070731653e-09]\n",
      "[1.0256865439898213e-32]\n",
      "[1.2262073157385077e-53]\n",
      "[1.087910662302161e-17]\n",
      "[1.2195337805982807e-65]\n",
      "[2.8764812399864285e-21]\n",
      "[2.3793938196681386e-07]\n",
      "[1.4876590664058873e-63]\n",
      "[1.1842285433657872e-13]\n",
      "[1.2195337805982807e-65]\n",
      "[3.1216933445739724e-18]\n",
      "[1.2195337805982807e-65]\n",
      "[7.3834782945791242e-24]\n",
      "[1.2195337805982807e-65]\n",
      "[6.1384762030623144e-87]\n",
      "[9.5565226916057872e-77]\n",
      "[1.6405167702172212e-44]\n",
      "[8.1066201009706166e-35]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teste = pd.DataFrame({'Teste': df2['Teste']})\n",
    "                            \n",
    "teste['Tokenized_teste'] = teste.apply(lambda row: nltk.word_tokenize(row['Teste']), axis=1)\n",
    "\n",
    "relevante_irrelevante = []\n",
    "\n",
    "tweets = []\n",
    "\n",
    "\n",
    "for tweet in teste['Tokenized_teste']:\n",
    "    tweets.append(tweet)\n",
    "\n",
    "        \n",
    "def multiply(numbers):  \n",
    "    total = 1\n",
    "    for x in numbers:\n",
    "        total *= x  \n",
    "    return total\n",
    "\n",
    "classificacao_teste = []\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    sentenca_relevante = []\n",
    "    sentenca_irrelevante = []\n",
    "    irrelevante = []\n",
    "    for j in tweets[i]:\n",
    "        if j in frequencia:\n",
    "            sentenca_relevante.append(((frequencia[j])/(len(all_words)))*prob_rel_irr[1])\n",
    "        if j not in frequencia:\n",
    "            sentenca_relevante.append((1/(len(all_words) + (len(frequencia)+len(frequencia2))))*prob_rel_irr[1])\n",
    "            \n",
    "        if j in frequencia2:\n",
    "            sentenca_irrelevante.append(((frequencia2[j])/(len(all_words)))*prob_rel_irr[0])\n",
    "        if j not in frequencia2:\n",
    "            sentenca_irrelevante.append((1/(len(all_words) + (len(frequencia)+len(frequencia2))))*prob_rel_irr[0])\n",
    "        \n",
    "    \n",
    "    relevante = multiply(sentenca_relevante)\n",
    "    irrelevante.append(multiply(sentenca_irrelevante))\n",
    "    print(irrelevante)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if irrelevante<relevante:\n",
    "        classificacao_teste.append('Relevante')\n",
    "    else:\n",
    "        classificacao_teste.append('Irrelevante')\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "3.02587633966e-58 < 9.55652269161e-77\n",
    "\n",
    "        \n",
    "        \n",
    "            \n",
    "                                  \n",
    "\n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "    \n",
    "#prob_palavra_relevante = ((frequencia[j]+1)/(len(all_words) + (len(frequencia)+len(frequencia2))))*prob_rel_irr[1]\n",
    "#prob_palavra_irrelevante = ((frequencia2[j]+1)/(len(all_words) + (len(frequencia)+len(frequencia2))))*prob_rel_irr[0]\n",
    "    \n",
    "    #relevante_irrelevante.append(' '.join(tweet))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "___\n",
    "## Concluindo\n",
    "\n",
    "Escreva aqui a sua conclusão.<br /> \n",
    "Faça um comparativo qualitativo sobre as medidas obtidas.<br />\n",
    "Explique como são tratadas as mensagens com dupla negação e sarcasmo.<br />\n",
    "Proponha um plano de expansão. Por que eles devem continuar financiando o seu projeto?<br />\n",
    "\n",
    "Opcionalmente: \n",
    "* Discorrer por que não posso alimentar minha base de Treinamento automaticamente usando o próprio classificador, aplicado a novos tweets.\n",
    "* Propor diferentes cenários de uso para o classificador Naive-Bayes. Cenários sem intersecção com este projeto.\n",
    "* Sugerir e explicar melhorias reais no classificador com indicações concretas de como implementar (não é preciso codificar, mas indicar como fazer e material de pesquisa sobre o assunto).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
